{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurvale/TCC-2026/blob/main/predicao_velocidade_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjuNEgKj6ixq"
      },
      "source": [
        "# Predição de Velocidade Veicular em Redes 5G - Machine Learning e Deep Learning\n",
        "\n",
        "**Autor:** Arthur Vale Fonseca\n",
        "\n",
        "**Data:** Novembro 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKpgT9yF6ixu"
      },
      "source": [
        "# 1. Importação de Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCuY-Lw76ixv"
      },
      "outputs": [],
      "source": [
        "# Bibliotecas básicas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Explicação dos modelos\n",
        "!pip install shap\n",
        "import shap\n",
        "\n",
        "# Fine Tunning\n",
        "!pip install optuna -q\n",
        "from optuna.samplers import TPESampler\n",
        "import optuna\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwo0W1JB6ixw"
      },
      "source": [
        "# 2. Carregamento dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSIRcVXM6ixw"
      },
      "outputs": [],
      "source": [
        "# Ajustar o caminho\n",
        "try:\n",
        "  df = pd.read_parquet('/content/processed_df.parquet')\n",
        "except:\n",
        "  print(\"Arquivo não encontrado, ajustar o caminho\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"INFORMAÇÕES DO DATASET\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Dimensões: {df.shape}\")\n",
        "print(f\"\\nPrimeiras linhas:\")\n",
        "display(df.head())\n",
        "\n",
        "print(f\"\\nEstatísticas da variável alvo (Speed):\")\n",
        "display(df['Speed'].describe())\n"
      ],
      "metadata": {
        "id": "qXa2Beff_HNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "KNbIzqzSPqAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "TGYr7GDyRIiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. FILTRAGEM DO MELHOR SINAL\n"
      ],
      "metadata": {
        "id": "vB9fropHH4KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"2.1. FILTRAGEM DO MELHOR SINAL (MAIOR SS_PBCH-RSRP)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Seleciona o melhor RSRP a cada instante. Mas, se houver algum empate o RSRQ é analisado.\")\n",
        "df['timestamp'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d.%m.%Y %H:%M:%S.%f')\n",
        "df_best = df.sort_values(['SS_PBCH-RSRP', 'SS_PBCH-RSRQ'], ascending=[False, False]).groupby('timestamp').first().reset_index()\n",
        "print(f\"Após filtragem: {df_best.shape}\")\n"
      ],
      "metadata": {
        "id": "KzLDpyolH6qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_best.shape"
      ],
      "metadata": {
        "id": "li1LAAFVegBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Criação de novas features\n"
      ],
      "metadata": {
        "id": "BjDcy6p9KMA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_best['timestamp'] = pd.to_datetime(df_best['timestamp'])\n",
        "df_best['Date'] = pd.to_datetime(df_best['Date'])\n",
        "\n",
        "df_best = df_best.sort_values('timestamp')\n",
        "\n",
        "deltas = df_best['timestamp'].diff().dt.total_seconds()\n",
        "print(deltas.describe())"
      ],
      "metadata": {
        "id": "UdeLcODY-pCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identificando segmentos continuos dentro de um mesmo dia.\n",
        "# se houver um intervalo maior do que 1 minuto entre 2 medidas entao a janela da media movel deve ser \"resetada\"\n",
        "\n",
        "df_best = df_best.sort_values('timestamp')\n",
        "\n",
        "df_best['delta_s'] = df_best.groupby('Date')['timestamp'].transform(lambda x: x.diff().dt.total_seconds())\n",
        "df_best['segment'] = df_best.groupby('Date')['delta_s'].transform(lambda s: (s > 60).cumsum())"
      ],
      "metadata": {
        "id": "Lhf7I5hSOjZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# media movel com janela de 10 segundos\n",
        "g = df_best.groupby(['Date', 'segment'])\n",
        "\n",
        "r = g.rolling('10s', on='timestamp', min_periods=1, closed='right')\n",
        "\n",
        "s = r['SS_PBCH-RSRP'].mean()\n",
        "\n",
        "s_df = (\n",
        "    s.rename('SS_PBCH-RSRP_janela_10_s')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "df_best = df_best.merge(\n",
        "    s_df,\n",
        "    on=['Date', 'segment', 'timestamp'],\n",
        "    how='left'\n",
        ")\n",
        "df_best.head()"
      ],
      "metadata": {
        "id": "r5a8Rl-rhdRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_best = df_best.sort_values('timestamp')\n",
        "\n",
        "df_idx = df_best.set_index('timestamp')\n",
        "\n",
        "per_10s = (\n",
        "    df_idx\n",
        "      .groupby(['Date', 'segment'])\n",
        "      .resample('10S')\n",
        "      .size()\n",
        "      .rename('samples_in_bin_10s')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "per_10s['samples_in_bin_10s'].describe()\n"
      ],
      "metadata": {
        "id": "m47jHLzApZvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "unique_days = sorted(df_best['timestamp'].dt.date.unique())\n",
        "\n",
        "for day in unique_days:\n",
        "    df_day = df_best[df_best['timestamp'].dt.date == day]\n",
        "\n",
        "    df_day = df_day.sort_values('timestamp')\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.scatter(\n",
        "        df_day['timestamp'],\n",
        "        df_day['SS_PBCH-RSRP'],\n",
        "        s=6,\n",
        "        alpha=0.6,\n",
        "        label='RSRP (medido)'\n",
        "    )\n",
        "\n",
        "    plt.plot(\n",
        "        df_day['timestamp'],\n",
        "        df_day['SS_PBCH-RSRP_janela_10_s'],\n",
        "        linewidth=0.5,\n",
        "        color='green',\n",
        "        label='RSRP média móvel 10s'\n",
        "    )\n",
        "\n",
        "    xmin = df_day['timestamp'].min()\n",
        "    xmax = df_day['timestamp'].max()\n",
        "    plt.xlim([xmin, xmax])\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
        "\n",
        "    plt.xlabel(\"Timestamp\")\n",
        "    plt.ylabel(\"RSRP (dBm)\")\n",
        "    plt.title(f\"RSRP do Canal SS_PBCH e média móvel de 10s — {day}\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "    filename = f\"{day}_rsrp_media_movel_10s.png\"\n",
        "    plt.savefig(filename, dpi=300)\n",
        "    files.download(filename)\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "NHNhEpmttYge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# media movel com janela de 5 segundos\n",
        "g = df_best.groupby(['Date', 'segment'])\n",
        "\n",
        "r = g.rolling('5s', on='timestamp', min_periods=1, closed='right')\n",
        "\n",
        "s = r['SS_PBCH-RSRP'].mean()\n",
        "\n",
        "s_df = (\n",
        "    s.rename('SS_PBCH-RSRP_janela_5_s')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "df_best = df_best.merge(\n",
        "    s_df,\n",
        "    on=['Date', 'segment', 'timestamp'],\n",
        "    how='left'\n",
        ")\n",
        "df_best.head()"
      ],
      "metadata": {
        "id": "SUB0dIwCvibV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_days = sorted(df_best['timestamp'].dt.date.unique())\n",
        "\n",
        "for day in unique_days:\n",
        "    df_day = df_best[df_best['timestamp'].dt.date == day]\n",
        "\n",
        "    df_day = df_day.sort_values('timestamp')\n",
        "\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.scatter(\n",
        "        df_day['timestamp'],\n",
        "        df_day['SS_PBCH-RSRP'],\n",
        "        s=6,\n",
        "        alpha=0.6,\n",
        "        label='RSRP (medido)'\n",
        "    )\n",
        "\n",
        "    plt.plot(\n",
        "        df_day['timestamp'],\n",
        "        df_day['SS_PBCH-RSRP_janela_5_s'],\n",
        "        linewidth=0.5,\n",
        "        color='green',\n",
        "        label='RSRP média móvel 5s'\n",
        "    )\n",
        "\n",
        "    xmin = df_day['timestamp'].min()\n",
        "    xmax = df_day['timestamp'].max()\n",
        "    plt.xlim([xmin, xmax])\n",
        "\n",
        "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
        "\n",
        "    plt.xlabel(\"Timestamp\")\n",
        "    plt.ylabel(\"RSRP (dBm)\")\n",
        "    plt.title(f\"RSRP do Canal SS_PBCH e média móvel de 5s — {day}\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "    filename = f\"{day}_rsrp_media_movel_5s.png\"\n",
        "    plt.savefig(filename, dpi=300)\n",
        "    files.download(filename)\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "MlMRS54Rvr-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_5s = (\n",
        "    df_idx\n",
        "      .groupby(['Date', 'segment'])\n",
        "      .resample('5S')\n",
        "      .size()\n",
        "      .rename('samples_in_bin_5s')\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "display(per_5s['samples_in_bin_5s'].describe())\n",
        "per_5s.head(20)"
      ],
      "metadata": {
        "id": "Ntn6SrUlslvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_colunas = ['SSS', 'DM_RS', 'PBCH', 'PSS']\n",
        "pos_colunas=['-RSRP', '-RSRQ', '-SINR', '-RePower']\n",
        "for pre in pre_colunas:\n",
        "    for pos in pos_colunas:\n",
        "      for window in [5, 10]:\n",
        "\n",
        "        g = df_best.groupby(['Date', 'segment'])\n",
        "        tempo=f'{window}s'\n",
        "        r = g.rolling(tempo, on='timestamp', min_periods=1, closed='right')\n",
        "        nome_coluna=f'{pre}{pos}'\n",
        "        nome_colunaFinal=f'{pre}{pos}__janela_{window}_s'\n",
        "\n",
        "\n",
        "        s = r[nome_coluna].mean()\n",
        "\n",
        "        s_df = (\n",
        "            s.rename(nome_colunaFinal)\n",
        "              .reset_index()\n",
        "        )\n",
        "\n",
        "        df_best = df_best.merge(\n",
        "            s_df,\n",
        "            on=['Date', 'segment', 'timestamp'],\n",
        "            how='left'\n",
        "        )"
      ],
      "metadata": {
        "id": "8hc_TTIKz6Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre1_col=['SS_PBCH']\n",
        "pos1_col=['-RSRQ', '-SINR', '-RePower']\n",
        "\n",
        "df_best = df_best.sort_values('timestamp')\n",
        "\n",
        "for pre in pre1_col:\n",
        "    for pos in pos1_col:\n",
        "      for window in [5, 10]:\n",
        "\n",
        "        g = df_best.groupby(['Date', 'segment'])\n",
        "        tempo=f'{window}s'\n",
        "        r = g.rolling(tempo, on='timestamp', min_periods=1, closed='right')\n",
        "        nome_coluna=f'{pre}{pos}'\n",
        "        nome_colunaFinal=f'{pre}{pos}__janela_{window}_s'\n",
        "\n",
        "\n",
        "        s = r[nome_coluna].mean()\n",
        "\n",
        "        s_df = (\n",
        "            s.rename(nome_colunaFinal)\n",
        "              .reset_index()\n",
        "        )\n",
        "\n",
        "        df_best = df_best.merge(\n",
        "            s_df,\n",
        "            on=['Date', 'segment', 'timestamp'],\n",
        "            how='left'\n",
        "        )"
      ],
      "metadata": {
        "id": "YMYsUKvdKPTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_colunas = ['SSS', 'DM_RS', 'PBCH', 'PSS']\n",
        "pos_colunas=['-RSRP', '-RSRQ', '-SINR', '-RePower']\n",
        "for pre in pre_colunas:\n",
        "    for pos in pos_colunas:\n",
        "      nome_excluido=f'{pre}{pos}'\n",
        "      df_best = df_best.drop(nome_excluido, axis=1)\n",
        "\n",
        "pre1_col=['SS_PBCH']\n",
        "pos1_col=['-RSRQ', '-SINR', '-RePower']\n",
        "for pre in pre1_col:\n",
        "    for pos in pos1_col:\n",
        "      nome_excluido=f'{pre}{pos}'\n",
        "      df_best = df_best.drop(nome_excluido, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z_pBCmDmzxUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. FEATURES TEMPORAIS (HORA, DIA)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"6. FEATURES TEMPORAIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df_best['day_of_week'] = (df_best['timestamp'].dt.weekday < 5).astype(int)\n",
        "hour = df_best['timestamp'].dt.hour\n",
        "df_best['hour_sin'] = np.sin(2 * np.pi * hour / 24)\n",
        "df_best['hour_cos'] = np.cos(2 * np.pi * hour / 24)\n",
        "\n",
        "columns_to_drop = ['timestamp','Date','Time','UTC','SS_PBCH-RSRP','campaign','delta_s','segment']\n",
        "#columns_to_drop = ['Date','Time','UTC','SS_PBCH-RSRP','campaign','delta_s','segment']\n",
        "\n",
        "df_best1 = df_best.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "df_best1.info()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aYIOR3ag0g3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqlZIBVM6ixy"
      },
      "source": [
        "# 4. Preparação dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG0kNfSK6ixy"
      },
      "outputs": [],
      "source": [
        "col=df_best1.columns.to_list()\n",
        "col.remove('Speed')\n",
        "X = df_best1[col].copy()\n",
        "y = df_best1['Speed'].copy()\n",
        "\n",
        "\n",
        "print(f\"Features utilizadas: {len(col)}\")\n",
        "print(f\"Amostras: {len(X)}\")\n",
        "\n",
        "# Split train/test\n",
        "Xx_train, X_test, yy_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTreino: {Xx_train.shape[0]}, Teste: {X_test.shape[0]}\")\n",
        "\n",
        "Xx_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train/validação\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    Xx_train, yy_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTreino: {X_train.shape[0]}, Teste: {X_val.shape[0]}\")\n",
        "\n",
        "X_train"
      ],
      "metadata": {
        "id": "ubJqDdtaUFkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_columns = ['day_of_week', 'PCI', 'SSBIdx']\n",
        "\n",
        "columns_to_scale = [c for c in col if c not in excluded_columns]\n",
        "\n",
        "X_train_to_scale = X_train[columns_to_scale]\n",
        "X_train_excluded = X_train[excluded_columns]\n",
        "\n",
        "X_val_to_scale = X_val[columns_to_scale]\n",
        "X_val_excluded = X_val[excluded_columns]\n",
        "\n",
        "X_test_to_scale_final = X_test[columns_to_scale]\n",
        "X_test_excluded_final = X_test[excluded_columns]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train_scaled_array = scaler.fit_transform(X_train_to_scale)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=columns_to_scale, index=X_train.index)\n",
        "X_train_scaled = pd.concat([X_train_scaled, X_train_excluded], axis=1)\n",
        "\n",
        "X_val_scaled_array = scaler.transform(X_val_to_scale)\n",
        "X_val_scaled = pd.DataFrame(X_val_scaled_array, columns=columns_to_scale, index=X_val.index)\n",
        "X_val_scaled = pd.concat([X_val_scaled, X_val_excluded], axis=1)\n",
        "\n",
        "X_test_scaled_array = scaler.transform(X_test_to_scale_final)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=columns_to_scale, index=X_test.index)\n",
        "X_test_scaled = pd.concat([X_test_scaled, X_test_excluded_final], axis=1)"
      ],
      "metadata": {
        "id": "_d2WuPse7h8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMFgpvDC6ixz"
      },
      "source": [
        "# 5. Modelo 1: Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6oHqME46ix0"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 1: RANDOM FOREST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf2_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando Random Forest...\")\n",
        "rf2_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_rf = rf2_model.predict(X_val_scaled)\n",
        "\n",
        "rf_r2 = r2_score(y_val, y_pred_rf)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
        "rf_mae = mean_absolute_error(y_val, y_pred_rf)\n",
        "\n",
        "print(f\"\\nR²: {rf_r2:.4f}\")\n",
        "print(f\"RMSE: {rf_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {rf_mae:.4f} km/h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuIqYLUb6ix0"
      },
      "source": [
        "# 6. Modelo 2: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3bG-fbw6ix0"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 2: XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando XGBoost...\")\n",
        "xgb_model.fit(X_train_scaled, y_train, verbose=False)\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_val_scaled)\n",
        "\n",
        "xgb_r2 = r2_score(y_val, y_pred_xgb)\n",
        "xgb_rmse = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
        "xgb_mae = mean_absolute_error(y_val, y_pred_xgb)\n",
        "\n",
        "print(f\"\\nR²: {xgb_r2:.4f}\")\n",
        "print(f\"RMSE: {xgb_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {xgb_mae:.4f} km/h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tqpfuze6ix0"
      },
      "source": [
        "# 7. Modelo 3: LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1Bbq2KV6ix1"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 3: LIGHTGBM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando LightGBM...\")\n",
        "lgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_lgb = lgb_model.predict(X_val_scaled)\n",
        "\n",
        "lgb_r2 = r2_score(y_val, y_pred_lgb)\n",
        "lgb_rmse = np.sqrt(mean_squared_error(y_val, y_pred_lgb))\n",
        "lgb_mae = mean_absolute_error(y_val, y_pred_lgb)\n",
        "\n",
        "print(f\"\\nR²: {lgb_r2:.4f}\")\n",
        "print(f\"RMSE: {lgb_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {lgb_mae:.4f} km/h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_b5RXeZ6ix2"
      },
      "source": [
        "# 8. Comparação de Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewTUsFSq6ix2"
      },
      "outputs": [],
      "source": [
        "# Resumo de resultados\n",
        "results = pd.DataFrame({\n",
        "    'Modelo': ['Random Forest', 'XGBoost', 'LightGBM'],\n",
        "    'R²': [rf_r2, xgb_r2, lgb_r2],\n",
        "    'RMSE (km/h)': [rf_rmse, xgb_rmse, lgb_rmse],\n",
        "    'MAE (km/h)': [rf_mae, xgb_mae, lgb_mae]\n",
        "})\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPARAÇÃO DE MODELOS\")\n",
        "print(\"=\"*80)\n",
        "display(results.style.highlight_max(axis=0, subset=['R²']).highlight_min(axis=0, subset=['RMSE (km/h)', 'MAE (km/h)']))\n",
        "\n",
        "# Visualização\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# R² Score\n",
        "axes[0].bar(results['Modelo'], results['R²'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "axes[0].set_title('Coeficiente de Determinação (R²)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('R²')\n",
        "axes[0].set_ylim(0, 1)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "for i, v in enumerate(results['R²']):\n",
        "    axes[0].text(i, v + 0.02, f\"{v:.4f}\", ha='center', fontweight='bold')\n",
        "\n",
        "# RMSE\n",
        "axes[1].bar(results['Modelo'], results['RMSE (km/h)'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7'])\n",
        "axes[1].set_title('Erro Quadrático Médio da Raiz (RMSE)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('RMSE (km/h)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "for i, v in enumerate(results['RMSE (km/h)']):\n",
        "    axes[1].text(i, v + 0.5, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Explicação da tomada de decisão do modelo XGBM (melhor modelo)"
      ],
      "metadata": {
        "id": "RAy-L0Zu19Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.Explainer(xgb_model)\n",
        "shap_values = explainer(X_train_scaled)\n",
        "\n",
        "shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "id": "e1Q2eZjJRRNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "shap.summary_plot(shap_values, X_train_scaled, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "O4oFsvuMRXoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap_array = np.abs(shap_values.values)\n",
        "\n",
        "shap_importance = shap_array.mean(axis=0)\n",
        "\n",
        "df_shap_importance = pd.DataFrame({\n",
        "    \"Feature\": X_train_scaled.columns,\n",
        "    \"Mean |SHAP value|\": shap_importance\n",
        "}).sort_values(\"Mean |SHAP value|\", ascending=False)\n",
        "\n",
        "df_shap_importance.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_shap_importance"
      ],
      "metadata": {
        "id": "6kZcHqrD4eIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A análise de explicabilidade via SHAP revelou que as variáveis Latitude e Longitude são, de forma destacada, os dois descritores com maior impacto sobre as previsões de velocidade. Isso indica que o modelo está capturando fortemente padrões geoespaciais específicos do conjunto de treino. Embora isso aumente o desempenho no conjunto atual de dados, também revela um risco importante:\n",
        "\n",
        "O modelo pode estar se apoiando em coordenadas geográficas absolutas, pouco generalizáveis para novas regiões.\n",
        "\n",
        "Em outras palavras, o algoritmo aprende características muito particulares de cada ponto no mapa (por exemplo, ruas, quadras, morros, prédios), tornando-se dependente do local exato onde as medições foram realizadas. Esse comportamento reduz a capacidade do modelo de generalizar para áreas diferentes ou campanhas futuras.\n",
        "\n",
        "Para mitigar esse efeito e aumentar a robustez do modelo, optei por transformar as variáveis de latitude e longitude em um descritor categórico de contexto geográfico, classificando cada ponto como:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Urbano Densa\n",
        "*   Suburbano\n",
        "*   Via Expressa\n",
        "* outras\n",
        "\n",
        "Essa abordagem substitui coordenadas exatas (altamente específicas e sensíveis ao dataset) por tipos de região, que são muito mais generalizáveis. Assim, o modelo passa a aprender características associadas ao tipo de ambiente — como densidade de edificações, nível de obstrução, morfologia urbana e comportamento esperado do sinal — em vez de depender de coordenadas específicas.\n",
        "\n",
        "\n",
        "Essa transformação melhora a capacidade do algoritmo de generalizar para novas cidades, novas campanhas e diferentes contextos espaciais, mantendo a utilidade da informação geográfica de forma mais abstrata e robusta.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J45dmGix42Oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Transformação das variaveis adicionais"
      ],
      "metadata": {
        "id": "5BqRAgg16Pw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Engenharia de atributos determinada heuristicamente"
      ],
      "metadata": {
        "id": "uJi9TZii8o5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_environment(row):\n",
        "    lat = row['Latitude']\n",
        "    lon = row['Longitude']\n",
        "    speed = row['Speed']\n",
        "    ts = row['timestamp']\n",
        "\n",
        "    dias_centro = [\n",
        "        pd.to_datetime(\"2020-12-14\").date(),\n",
        "        pd.to_datetime(\"2020-12-17\").date(),\n",
        "        pd.to_datetime(\"2020-12-22\").date()\n",
        "    ]\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # ÁREA VERDE\n",
        "    # ----------------------------------------\n",
        "    ts_area_verde_ini = pd.Timestamp(\"2021-01-11 16:09:46.868000\")\n",
        "    ts_area_verde_fim = pd.Timestamp(\"2021-01-11 16:24:41.269000\")\n",
        "\n",
        "    ts_area_verde_2_ini = pd.Timestamp(\"2020-12-22 15:27:53.268000\")\n",
        "    ts_area_verde_2_fim = pd.Timestamp(\"2020-12-22 15:28:29.398000\")\n",
        "\n",
        "    ts_area_verde_3_ini = pd.Timestamp(\"2021-01-10 18:20:26.241000\")\n",
        "    ts_area_verde_3_fim = pd.Timestamp(\"2021-01-10 18:21:31.720000\")\n",
        "\n",
        "    ts_area_verde_4_ini = pd.Timestamp(\"2021-01-10 18:09:03.839000\")\n",
        "    ts_area_verde_4_fim = pd.Timestamp(\"2021-01-10 18:09:51.712000\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # VIA RÁPIDA\n",
        "    # ----------------------------------------\n",
        "    ts_via_rapida_ini = pd.Timestamp(\"2021-01-11 15:45:57.169000\")\n",
        "    ts_via_rapida_fim = pd.Timestamp(\"2021-01-11 16:08:38.159000\")\n",
        "\n",
        "    ts_vr_extra_1_ini = pd.Timestamp(\"2021-01-11 16:06:15.349000\")\n",
        "    ts_vr_extra_1_fim = pd.Timestamp(\"2021-01-11 16:06:36.159000\")\n",
        "\n",
        "    ts_vr_extra_2_single = pd.Timestamp(\"2021-01-02 12:07:07.329000\")\n",
        "    ts_vr_extra_3_single = pd.Timestamp(\"2021-01-11 15:57:35.431000\")\n",
        "\n",
        "    ts_vr_extra_4_ini = pd.Timestamp(\"2021-01-02 11:57:08.862000\")\n",
        "    ts_vr_extra_4_fim = pd.Timestamp(\"2021-01-02 11:57:30.871000\")\n",
        "\n",
        "    ts_vr_extra_5_ini = pd.Timestamp(\"2021-01-08 16:18:12.344000\")\n",
        "    ts_vr_extra_5_fim = pd.Timestamp(\"2021-01-08 16:18:27.863000\")\n",
        "\n",
        "    ts_vr_extra_6_single = pd.Timestamp(\"2021-01-08 17:59:17.008000\")\n",
        "    ts_vr_extra_7_single = pd.Timestamp(\"2021-01-02 12:08:27.609000\")\n",
        "\n",
        "    ts_vr_extra_8_ini = pd.Timestamp(\"2021-01-11 15:37:44.363000\")\n",
        "    ts_vr_extra_8_fim = pd.Timestamp(\"2021-01-11 15:42:43.909000\")\n",
        "\n",
        "    ts_vr_extra_9_ini = pd.Timestamp(\"2021-01-02 11:49:03.646000\")\n",
        "    ts_vr_extra_9_fim = pd.Timestamp(\"2021-01-02 11:51:12.959000\")\n",
        "\n",
        "\n",
        "    ts_vr_extra_11_ini = pd.Timestamp(\"2021-01-08 18:06:46.780000\")\n",
        "    ts_vr_extra_11_fim = pd.Timestamp(\"2021-01-08 18:09:06.653000\")\n",
        "    ts_vr_extra_12_ini = pd.Timestamp(\"2021-01-08 16:02:49.133000\")\n",
        "    ts_vr_extra_12_fim = pd.Timestamp(\"2021-01-08 16:10:49.128000\")\n",
        "\n",
        "    ts_vr_extra_12_single = pd.Timestamp(\"2021-01-08 16:18:09.766000\")\n",
        "\n",
        "    ts_vr_extra_13_ini = pd.Timestamp(\"2021-01-02 12:15:01.582000\")\n",
        "    ts_vr_extra_13_fim = pd.Timestamp(\"2021-01-02 12:17:10.200000\")\n",
        "\n",
        "    # ----------------------------------------\n",
        "    # RESIDENCIAL\n",
        "    # ----------------------------------------\n",
        "    ts_resid_1_ini = pd.Timestamp(\"2021-01-16 16:56:20.352000\")\n",
        "    ts_resid_1_fim = pd.Timestamp(\"2021-01-16 16:58:16.520000\")\n",
        "\n",
        "    ts_resid_2_ini = pd.Timestamp(\"2021-01-11 15:50:11.988000\")\n",
        "    ts_resid_2_fim = pd.Timestamp(\"2021-01-11 15:59:01.937000\")\n",
        "\n",
        "    ts_resid_3_ini = pd.Timestamp(\"2020-12-14 19:18:04.782000\")\n",
        "    ts_resid_3_fim = pd.Timestamp(\"2020-12-14 19:21:27.236000\")\n",
        "\n",
        "    ts_resid_4_ini = pd.Timestamp(\"2020-12-17 18:38:16.028000\")\n",
        "    ts_resid_4_fim = pd.Timestamp(\"2020-12-17 18:43:38.342000\")\n",
        "\n",
        "    ts_resid_5_ini = pd.Timestamp(\"2020-12-22 15:40:30.182000\")\n",
        "    ts_resid_5_fim = pd.Timestamp(\"2020-12-22 15:50:28.872000\")\n",
        "\n",
        "    ts_resid_6_ini = pd.Timestamp(\"2021-01-02 12:07:07.329000\")\n",
        "\n",
        "\n",
        "    if (\n",
        "        (ts_resid_1_ini <= ts <= ts_resid_1_fim) or\n",
        "        (ts_resid_2_ini <= ts <= ts_resid_2_fim) or\n",
        "        (ts_resid_3_ini <= ts <= ts_resid_3_fim) or\n",
        "        (ts_resid_4_ini <= ts <= ts_resid_4_fim) or\n",
        "        (ts_resid_5_ini <= ts <= ts_resid_5_fim) or\n",
        "        (ts==ts_resid_6_ini)\n",
        "\n",
        "    ):\n",
        "        return \"Residencial\"\n",
        "\n",
        "    if (\n",
        "        (ts_area_verde_ini <= ts <= ts_area_verde_fim) or\n",
        "        (ts_area_verde_2_ini <= ts <= ts_area_verde_2_fim) or\n",
        "        (ts_area_verde_3_ini <= ts <= ts_area_verde_3_fim) or\n",
        "        (ts_area_verde_4_ini <= ts <= ts_area_verde_4_fim)\n",
        "    ):\n",
        "        return \"Area_Verde\"\n",
        "\n",
        "\n",
        "    if ts.date() in dias_centro:\n",
        "        return \"Centro_historico\"\n",
        "\n",
        "    cond_via_rapida_coords = (\n",
        "        (41.8240953 <= lat <= 41.859721) and\n",
        "        (12.4661876 <= lon <= 12.4953271)\n",
        "    )\n",
        "\n",
        "    cond_via_rapida_time = (\n",
        "        (ts_via_rapida_ini <= ts <= ts_via_rapida_fim) or\n",
        "        (ts_vr_extra_1_ini <= ts <= ts_vr_extra_1_fim) or\n",
        "        (ts == ts_vr_extra_2_single) or\n",
        "        (ts == ts_vr_extra_3_single) or\n",
        "        (ts_vr_extra_4_ini <= ts <= ts_vr_extra_4_fim) or\n",
        "        (ts_vr_extra_5_ini <= ts <= ts_vr_extra_5_fim) or\n",
        "        (ts == ts_vr_extra_6_single) or\n",
        "        (ts == ts_vr_extra_7_single) or\n",
        "        (ts_vr_extra_8_ini <= ts <= ts_vr_extra_8_fim) or\n",
        "        (ts_vr_extra_9_ini <= ts <= ts_vr_extra_9_fim) or\n",
        "        #(ts_vr_extra_10_ini <= ts <= ts_vr_extra_10_fim) or\n",
        "        (ts_vr_extra_11_ini <= ts <= ts_vr_extra_11_fim)or\n",
        "        (ts_vr_extra_12_ini <= ts <= ts_vr_extra_12_fim)or\n",
        "        (ts == ts_vr_extra_12_single) or\n",
        "        (ts_vr_extra_13_ini <= ts <= ts_vr_extra_13_fim)\n",
        "    )\n",
        "\n",
        "    if cond_via_rapida_coords or cond_via_rapida_time:\n",
        "        return \"via_rapida\"\n",
        "\n",
        "    if (41.888 <= lat <= 41.901) and (12.470 <= lon <= 12.495):\n",
        "        return \"Centro_historico\"\n",
        "\n",
        "    return \"Residencial\""
      ],
      "metadata": {
        "id": "tbomlVKlXwmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_best['environment'] = df_best.apply(classify_environment, axis=1)"
      ],
      "metadata": {
        "id": "UoTYks4b-eNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_best['environment'].value_counts()"
      ],
      "metadata": {
        "id": "Qb4xa3-fY9Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium import Tooltip\n",
        "\n",
        "color_map = {\n",
        "    'via_rapida': 'red',\n",
        "    'Centro_historico': 'blue',\n",
        "    'Area_Verde': 'green',\n",
        "    'Residencial': 'purple'\n",
        "}\n",
        "\n",
        "map_center = [df_best['Latitude'].mean(), df_best['Longitude'].mean()]\n",
        "m = folium.Map(location=map_center, zoom_start=12)\n",
        "\n",
        "for idx, row in df_best.iterrows():\n",
        "\n",
        "    html = f\"\"\"\n",
        "        <div style=\"font-size:14px; font-weight:bold; width:250px;\">\n",
        "            <b>Latitude:</b> {row['Latitude']}<br>\n",
        "            <b>Longitude:</b> {row['Longitude']}<br>\n",
        "            <b>Timestamp:</b> {row['timestamp']}<br>\n",
        "            <b>Ambiente:</b> {row['environment']}\n",
        "        </div>\n",
        "    \"\"\"\n",
        "\n",
        "    tooltip = Tooltip(html, sticky=True, style=\"background-color:white;\")\n",
        "\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Latitude'], row['Longitude']],\n",
        "        radius=4,\n",
        "        color=color_map.get(row['environment'], 'gray'),\n",
        "        fill=True,\n",
        "        fill_color=color_map.get(row['environment'], 'gray'),\n",
        "        fill_opacity=0.7,\n",
        "        tooltip=tooltip\n",
        "    ).add_to(m)\n",
        "\n",
        "m"
      ],
      "metadata": {
        "id": "C5BOwMm9E-Hi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Modelagem 2:  com a feature enviroment"
      ],
      "metadata": {
        "id": "qsqu1Jl6wp34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelagem sem latitude x longitude, apenas com os rotulos referentes aos ambientes"
      ],
      "metadata": {
        "id": "M2B6OLJwxDzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['Latitude','Longitude','timestamp','Date','Time','UTC','SS_PBCH-RSRP','campaign','delta_s','segment']\n",
        "\n",
        "df_best2 = df_best.drop(columns=columns_to_drop, axis=1)\n",
        "\n",
        "df_best2.info()"
      ],
      "metadata": {
        "id": "siYdbChHn_Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "codigos = {\n",
        "    'Residencial': 1,\n",
        "    'Centro_historico': 2,\n",
        "    'via_rapida': 3,\n",
        "    'Area_Verde': 4\n",
        "}\n",
        "\n",
        "df_best2[\"environment\"] = df_best2[\"environment\"].map(codigos)"
      ],
      "metadata": {
        "id": "5ByeRTK_rXPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Antes da transformação:\")\n",
        "display(df_best2['environment'].dtype)\n",
        "display(df_best2['environment'].value_counts())\n",
        "\n",
        "df_best2['environment'] = df_best2['environment'].astype('category')\n",
        "\n",
        "print(\"\\nDepois da transformação:\")\n",
        "display(df_best2['environment'].dtype)\n",
        "display(df_best2['environment'].value_counts())"
      ],
      "metadata": {
        "id": "zXmIhIQMnyVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col=df_best2.columns.to_list()\n",
        "col.remove('Speed')\n",
        "X = df_best2[col].copy()\n",
        "y = df_best2['Speed'].copy()\n",
        "\n",
        "\n",
        "print(f\"Features utilizadas: {len(col)}\")\n",
        "print(f\"Amostras: {len(X)}\")\n",
        "\n",
        "# Split train/test\n",
        "Xx_train, X_test, yy_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTreino (total): {Xx_train.shape[0]}, Teste: {X_test.shape[0]}\")\n",
        "\n",
        "# Split train/validação\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    Xx_train, yy_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nTreino: {X_train.shape[0]}, Validação: {X_val.shape[0]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "st5pp4Brta24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "excluded_non_transformed_columns = ['day_of_week', 'PCI', 'SSBIdx']\n",
        "one_hot_encode_columns = ['environment']\n",
        "\n",
        "columns_to_scale = [c for c in col if c not in excluded_non_transformed_columns + one_hot_encode_columns]\n",
        "\n",
        "\n",
        "X_train_to_scale = X_train[columns_to_scale]\n",
        "X_train_to_onehot = X_train[one_hot_encode_columns]\n",
        "X_train_non_transformed = X_train[excluded_non_transformed_columns]\n",
        "\n",
        "X_val_to_scale = X_val[columns_to_scale]\n",
        "X_val_to_onehot = X_val[one_hot_encode_columns]\n",
        "X_val_non_transformed = X_val[excluded_non_transformed_columns]\n",
        "\n",
        "X_test_to_scale_final = X_test[columns_to_scale]\n",
        "X_test_to_onehot_final = X_test[one_hot_encode_columns]\n",
        "X_test_non_transformed_final = X_test[excluded_non_transformed_columns]\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled_array = scaler.fit_transform(X_train_to_scale)\n",
        "X_val_scaled_array = scaler.transform(X_val_to_scale)\n",
        "X_test_scaled_array = scaler.transform(X_test_to_scale_final)\n",
        "\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled_array, columns=columns_to_scale, index=X_train.index)\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled_array, columns=columns_to_scale, index=X_val.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled_array, columns=columns_to_scale, index=X_test.index)\n",
        "\n",
        "\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "X_train_ohe_array = ohe.fit_transform(X_train_to_onehot)\n",
        "X_val_ohe_array = ohe.transform(X_val_to_onehot)\n",
        "X_test_ohe_array = ohe.transform(X_test_to_onehot_final)\n",
        "\n",
        "ohe_feature_names = ohe.get_feature_names_out(one_hot_encode_columns)\n",
        "\n",
        "X_train_ohe_df = pd.DataFrame(X_train_ohe_array, columns=ohe_feature_names, index=X_train.index)\n",
        "X_val_ohe_df = pd.DataFrame(X_val_ohe_array, columns=ohe_feature_names, index=X_val.index)\n",
        "X_test_ohe_df = pd.DataFrame(X_test_ohe_array, columns=ohe_feature_names, index=X_test.index)\n",
        "\n",
        "\n",
        "X_train_scaled = pd.concat([X_train_scaled_df, X_train_ohe_df, X_train_non_transformed], axis=1)\n",
        "X_val_scaled = pd.concat([X_val_scaled_df, X_val_ohe_df, X_val_non_transformed], axis=1)\n",
        "X_test_scaled = pd.concat([X_test_scaled_df, X_test_ohe_df, X_test_non_transformed_final], axis=1)"
      ],
      "metadata": {
        "id": "SZ8N5YHAt1HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.2. RF"
      ],
      "metadata": {
        "id": "Pwtw38sst9mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 1: RANDOM FOREST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf3_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando Random Forest...\")\n",
        "rf3_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_rf = rf3_model.predict(X_val_scaled)\n",
        "\n",
        "rf2_r2 = r2_score(y_val, y_pred_rf)\n",
        "rf2_rmse = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
        "rf2_mae = mean_absolute_error(y_val, y_pred_rf)\n",
        "\n",
        "print(f\"\\nR²: {rf2_r2:.4f}\")\n",
        "print(f\"RMSE: {rf2_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {rf2_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "R14mgGkauIGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.3 XGB"
      ],
      "metadata": {
        "id": "Zdn4Y6g0uYnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 2: XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "xgb2_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando XGBoost...\")\n",
        "xgb2_model.fit(X_train_scaled, y_train, verbose=False)\n",
        "\n",
        "y_pred_xgb = xgb2_model.predict(X_val_scaled)\n",
        "\n",
        "xgb2_r2 = r2_score(y_val, y_pred_xgb)\n",
        "xgb2_rmse = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
        "xgb2_mae = mean_absolute_error(y_val, y_pred_xgb)\n",
        "\n",
        "print(f\"\\nR²: {xgb2_r2:.4f}\")\n",
        "print(f\"RMSE: {xgb2_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {xgb2_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "UdQVilzGugkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11.4. LGBM"
      ],
      "metadata": {
        "id": "aAfmWGdPuzrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 3: LIGHTGBM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lgb2_model = lgb.LGBMRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando LightGBM...\")\n",
        "lgb2_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_lgb = lgb2_model.predict(X_val_scaled)\n",
        "\n",
        "lgb2_r2 = r2_score(y_val, y_pred_lgb)\n",
        "lgb2_rmse = np.sqrt(mean_squared_error(y_val, y_pred_lgb))\n",
        "lgb2_mae = mean_absolute_error(y_val, y_pred_lgb)\n",
        "\n",
        "print(f\"\\nR²: {lgb2_r2:.4f}\")\n",
        "print(f\"RMSE: {lgb2_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {lgb2_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "kaeEj9dMu23g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Modelagem 3:  com a feature enviroment e sem: altitude, latitude e longitude"
      ],
      "metadata": {
        "id": "WgPk7fQVxbBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val_scaled=X_val_scaled.drop(columns=['Altitude'], axis=1)\n",
        "X_train_scaled=X_train_scaled.drop(columns=['Altitude'], axis=1)"
      ],
      "metadata": {
        "id": "dAA1qGEoquFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.2: RF"
      ],
      "metadata": {
        "id": "YJQJ5iG7yHI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 1: RANDOM FOREST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "rf4_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando Random Forest...\")\n",
        "rf4_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_rf = rf4_model.predict(X_val_scaled)\n",
        "\n",
        "rf4_r2 = r2_score(y_val, y_pred_rf)\n",
        "rf4_rmse = np.sqrt(mean_squared_error(y_val, y_pred_rf))\n",
        "rf4_mae = mean_absolute_error(y_val, y_pred_rf)\n",
        "\n",
        "print(f\"\\nR²: {rf4_r2:.4f}\")\n",
        "print(f\"RMSE: {rf4_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {rf4_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "4C5CtzGpyI-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.3: XGBM"
      ],
      "metadata": {
        "id": "ocXbanR9ybe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 2: XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "xgb3_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando XGBoost...\")\n",
        "xgb3_model.fit(X_train_scaled, y_train, verbose=False)\n",
        "\n",
        "y_pred_xgb = xgb3_model.predict(X_val_scaled)\n",
        "\n",
        "xgb3_r2 = r2_score(y_val, y_pred_xgb)\n",
        "xgb3_rmse = np.sqrt(mean_squared_error(y_val, y_pred_xgb))\n",
        "xgb3_mae = mean_absolute_error(y_val, y_pred_xgb)\n",
        "\n",
        "print(f\"\\nR²: {xgb3_r2:.4f}\")\n",
        "print(f\"RMSE: {xgb3_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {xgb3_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "E4gKBS9wyfaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.4:LGBM"
      ],
      "metadata": {
        "id": "4-MNzF20ypi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODELO 3: LIGHTGBM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "lgb3_model = lgb.LGBMRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando LightGBM...\")\n",
        "lgb3_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred_lgb = lgb3_model.predict(X_val_scaled)\n",
        "\n",
        "lgb3_r2 = r2_score(y_val, y_pred_lgb)\n",
        "lgb3_rmse = np.sqrt(mean_squared_error(y_val, y_pred_lgb))\n",
        "lgb3_mae = mean_absolute_error(y_val, y_pred_lgb)\n",
        "\n",
        "print(f\"\\nR²: {lgb3_r2:.4f}\")\n",
        "print(f\"RMSE: {lgb3_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {lgb3_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "MC7ndVRmysNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Seleção de features"
      ],
      "metadata": {
        "id": "Nnjc02Es0Sqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.1: Explicação do modelo XGB treinado a partir da tomada de decisao das features"
      ],
      "metadata": {
        "id": "KyMRdM90e5PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.TreeExplainer(xgb3_model)\n",
        "n_shap = 2000\n",
        "X_shap = X_train_scaled.sample(n_shap, random_state=42)\n",
        "shap_values = explainer(X_shap)\n",
        "shap_array = np.abs(shap_values.values)\n",
        "\n",
        "# média absoluta por feature\n",
        "shap_importance = shap_array.mean(axis=0)\n",
        "\n",
        "df_shap_importance = pd.DataFrame({\n",
        "    \"Feature\": X_shap.columns,\n",
        "    \"Mean |SHAP value|\": shap_importance\n",
        "}).sort_values(\"Mean |SHAP value|\", ascending=False)\n",
        "\n",
        "df_shap_importance.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_shap_importance"
      ],
      "metadata": {
        "id": "A5bfWb9Bwyk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.2: Avaliação dos Modelos XGB com a remoção das features contextuais"
      ],
      "metadata": {
        "id": "Np3R2QvffG3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_enviroments=['environment_3','environment_2','environment_1','environment_4']\n",
        "X_train_scaled1=X_train_scaled.drop(columns=col_enviroments, axis=1)\n",
        "X_val_scaled1=X_val_scaled.drop(columns=col_enviroments, axis=1)\n",
        "\n",
        "col = ['PCI', 'hour_sin', 'hour_cos', 'day_of_week']\n",
        "\n",
        "modelos_xgb = {}\n",
        "predicoes_xgb = {}\n",
        "resultados = []\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENTO: REMOÇÃO PROGRESSIVA DE FEATURES CONTEXTUAIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for k in range(0, len(col) + 1):\n",
        "\n",
        "\n",
        "    if k == 0:\n",
        "        drop_cols = []\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"Com a remoção da feature Enviroments\")\n",
        "        print(\"-\"*80)\n",
        "    else:\n",
        "        drop_cols = col[:k]\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(f\"Removendo colunas Enviroments e: {drop_cols}\")\n",
        "        print(\"-\"*80)\n",
        "\n",
        "\n",
        "    if len(drop_cols) == 0:\n",
        "        X_train_curr = X_train_scaled1.copy()\n",
        "        X_val_curr   = X_val_scaled1.copy()\n",
        "    else:\n",
        "        X_train_curr = X_train_scaled1.drop(columns=drop_cols)\n",
        "        X_val_curr   = X_val_scaled1.drop(columns=drop_cols)\n",
        "\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=15,\n",
        "        learning_rate=0.05,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    print(\"\\nTreinando XGBoost...\")\n",
        "    xgb_model.fit(X_train_curr, y_train, verbose=False)\n",
        "\n",
        "\n",
        "    y_pred = xgb_model.predict(X_val_curr)\n",
        "\n",
        "\n",
        "    r2  = r2_score(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "\n",
        "    print(f\"R²:   {r2:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f} km/h\")\n",
        "    print(f\"MAE:  {mae:.4f} km/h\")\n",
        "\n",
        "\n",
        "    if k == 0:\n",
        "        model_name = \"xgb_sem_Enviroments\"\n",
        "    else:\n",
        "        model_name = f\"xgb_sem_Enviroments{'_'.join(drop_cols)}\"\n",
        "\n",
        "\n",
        "    modelos_xgb[model_name] = xgb_model\n",
        "    predicoes_xgb[model_name] = y_pred\n",
        "\n",
        "    resultados.append({\n",
        "        \"Modelo\": model_name,\n",
        "        \"Features_removidas\": drop_cols if drop_cols else \"Enviroments\",\n",
        "        \"R2\": r2,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae\n",
        "    })\n",
        "\n",
        "\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "df_resultados"
      ],
      "metadata": {
        "id": "-HjVLr5R9x6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Fine Tunning dos modelos XGB para diferentes configurações de features"
      ],
      "metadata": {
        "id": "r0IUsa4-faWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators_list = [80, 100, 120, 150]\n",
        "max_depth_list = [6, 8, 10, 12, 15]\n",
        "learning_rate_list = [0.03, 0.05, 0.07, 0.1]\n",
        "subsample_list = [0.6, 0.7, 0.8, 0.9]\n",
        "colsample_bytree_list = [0.6, 0.7, 0.8, 0.9]\n",
        "min_child_weight_list = [1, 2, 3, 5]\n",
        "gamma_list = [0.0, 0.1, 0.2, 0.3]\n",
        "reg_alpha_list = [0.0, 0.001, 0.01, 0.05, 0.1]\n",
        "reg_lambda_list = [1.0, 1.5, 2.0, 3.0]"
      ],
      "metadata": {
        "id": "O5SgGs04DGdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_params(params, X, y, n_splits=5, random_state=42):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    rmses, r2s = [], []\n",
        "\n",
        "    one_hot_encode_columns = ['environment']\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "        X_tr_raw  = X.iloc[train_idx].copy()\n",
        "        X_val_raw = X.iloc[val_idx].copy()\n",
        "        y_tr  = y.iloc[train_idx].copy()\n",
        "        y_val = y.iloc[val_idx].copy()\n",
        "\n",
        "        scaler = MinMaxScaler()\n",
        "        X_tr_scaled_num  = scaler.fit_transform(X_tr_raw[columns_to_scale])\n",
        "        X_val_scaled_num = scaler.transform(X_val_raw[columns_to_scale])\n",
        "\n",
        "        X_tr_scaled_df = pd.DataFrame(X_tr_scaled_num,\n",
        "                                      columns=columns_to_scale,\n",
        "                                      index=X_tr_raw.index)\n",
        "        X_val_scaled_df = pd.DataFrame(X_val_scaled_num,\n",
        "                                       columns=columns_to_scale,\n",
        "                                       index=X_val_raw.index)\n",
        "\n",
        "        try:\n",
        "            ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "        except TypeError:\n",
        "            ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "        X_tr_to_ohe  = X_tr_raw[one_hot_encode_columns]\n",
        "        X_val_to_ohe = X_val_raw[one_hot_encode_columns]\n",
        "\n",
        "        X_tr_ohe_array  = ohe.fit_transform(X_tr_to_ohe)\n",
        "        X_val_ohe_array = ohe.transform(X_val_to_ohe)\n",
        "\n",
        "        ohe_feature_names = ohe.get_feature_names_out(one_hot_encode_columns)\n",
        "\n",
        "        X_tr_ohe_df = pd.DataFrame(X_tr_ohe_array,\n",
        "                                   columns=ohe_feature_names,\n",
        "                                   index=X_tr_raw.index)\n",
        "        X_val_ohe_df = pd.DataFrame(X_val_ohe_array,\n",
        "                                    columns=ohe_feature_names,\n",
        "                                    index=X_val_raw.index)\n",
        "\n",
        "        X_tr_non = X_tr_raw[excluded_columns].drop(columns=one_hot_encode_columns, errors='ignore')\n",
        "        X_val_non = X_val_raw[excluded_columns].drop(columns=one_hot_encode_columns, errors='ignore')\n",
        "\n",
        "        X_tr_final  = pd.concat([X_tr_scaled_df,  X_tr_ohe_df,  X_tr_non], axis=1)\n",
        "        X_val_final = pd.concat([X_val_scaled_df, X_val_ohe_df, X_val_non], axis=1)\n",
        "\n",
        "        X_val_final = X_val_final.reindex(columns=X_tr_final.columns, fill_value=0)\n",
        "\n",
        "        xgb_model = xgb.XGBRegressor(\n",
        "            n_estimators=params['n_estimators'],\n",
        "            max_depth=params['max_depth'],\n",
        "            learning_rate=params['learning_rate'],\n",
        "            subsample=params['subsample'],\n",
        "            colsample_bytree=params['colsample_bytree'],\n",
        "            min_child_weight=params['min_child_weight'],\n",
        "            gamma=params['gamma'],\n",
        "            reg_alpha=params['reg_alpha'],\n",
        "            reg_lambda=params['reg_lambda'],\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            tree_method=\"hist\"\n",
        "        )\n",
        "\n",
        "        xgb_model.fit(X_tr_final, y_tr)\n",
        "        y_pred = xgb_model.predict(X_val_final)\n",
        "\n",
        "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        r2   = r2_score(y_val, y_pred)\n",
        "\n",
        "        rmses.append(rmse)\n",
        "        r2s.append(r2)\n",
        "\n",
        "    rmse_mean = np.mean(rmses)\n",
        "    rmse_std  = np.std(rmses)\n",
        "    r2_mean   = np.mean(r2s)\n",
        "    r2_std    = np.std(r2s)\n",
        "\n",
        "    return rmse_mean, rmse_std, r2_mean, r2_std"
      ],
      "metadata": {
        "id": "Y3jwwjCoDL7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    # amostra de hiperparâmetros\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_categorical('n_estimators', n_estimators_list),\n",
        "        'max_depth': trial.suggest_categorical('max_depth', max_depth_list),\n",
        "        'learning_rate': trial.suggest_categorical('learning_rate', learning_rate_list),\n",
        "        'subsample': trial.suggest_categorical('subsample', subsample_list),\n",
        "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', colsample_bytree_list),\n",
        "        'min_child_weight': trial.suggest_categorical('min_child_weight', min_child_weight_list),\n",
        "        'gamma': trial.suggest_categorical('gamma', gamma_list),\n",
        "        'reg_alpha': trial.suggest_categorical('reg_alpha', reg_alpha_list),\n",
        "        'reg_lambda': trial.suggest_categorical('reg_lambda', reg_lambda_list),\n",
        "    }\n",
        "\n",
        "    rmse_mean, rmse_std, r2_mean, r2_std = evaluate_params(params, Xx_train, yy_train)\n",
        "\n",
        "    # guardamos info adicional no trial\n",
        "    trial.set_user_attr(\"rmse_mean\", rmse_mean)\n",
        "    trial.set_user_attr(\"rmse_std\", rmse_std)\n",
        "    trial.set_user_attr(\"r2_mean\", r2_mean)\n",
        "    trial.set_user_attr(\"r2_std\", r2_std)\n",
        "\n",
        "    # ==========================\n",
        "    # Função objetivo:\n",
        "    # minimizar RMSE, mas com restrição R² >= 0.93\n",
        "    # ==========================\n",
        "    if r2_mean < 0.93:\n",
        "        # penaliza fortemente soluções com R² abaixo do limiar de 93%\n",
        "        penalty = (0.93 - r2_mean) * 50.0   # fator ajustável\n",
        "        objective_value = rmse_mean + penalty\n",
        "    else:\n",
        "        objective_value = rmse_mean\n",
        "\n",
        "    return objective_value"
      ],
      "metadata": {
        "id": "fWCibTH733O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14.1: Avaliando com todas as features contextuais"
      ],
      "metadata": {
        "id": "KqgqBYWdflg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_columns = ['day_of_week', 'PCI', 'SSBIdx','environment']\n",
        "\n",
        "all_features = Xx_train.columns.tolist()\n",
        "columns_to_scale = [c for c in all_features if c not in excluded_columns]"
      ],
      "metadata": {
        "id": "V9Ycx50wEltO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(\n",
        "    direction='minimize',\n",
        "    sampler=TPESampler(seed=42)\n",
        ")\n",
        "\n",
        "study.enqueue_trial({\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 15,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8\n",
        "})\n",
        "\n",
        "\n",
        "study.optimize(objective, timeout=3600)\n",
        "\n",
        "print(\"Melhores parâmetros encontrados:\")\n",
        "print(study.best_params)\n",
        "\n",
        "best_params = study.best_params\n",
        "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_params(best_params, Xx_train, yy_train)\n",
        "\n",
        "print(\"\\nDesempenho com melhores parâmetros (CV KFold):\")\n",
        "print(best_params)\n",
        "print(f\"RMSE médio: {rmse_mean:.4f} km/h\")\n",
        "print(f\"RMSE std:   {rmse_std:.4f} km/h\")\n",
        "print(f\"R² médio:   {r2_mean:.4f}\")\n",
        "print(f\"R² std:     {r2_std:.4f}\")"
      ],
      "metadata": {
        "id": "jCBsYF70pKI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14.2: Avaliando sem a feature de Altitude"
      ],
      "metadata": {
        "id": "O878A2XHgSLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xx_train = Xx_train.drop('Altitude',axis=1)\n",
        "excluded_columns = ['day_of_week', 'PCI', 'SSBIdx','environment']\n",
        "\n",
        "all_features = Xx_train.columns.tolist()\n",
        "columns_to_scale = [c for c in all_features if c not in excluded_columns]"
      ],
      "metadata": {
        "id": "xZQp-_ZH2ak3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(\n",
        "    direction='minimize',\n",
        "    sampler=TPESampler(seed=42)\n",
        ")\n",
        "\n",
        "study.enqueue_trial({\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 15,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8\n",
        "})\n",
        "\n",
        "\n",
        "study.optimize(objective, timeout=3600)\n",
        "\n",
        "print(\"Melhores parâmetros encontrados:\")\n",
        "print(study.best_params)\n",
        "\n",
        "best_params = study.best_params\n",
        "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_params(best_params, Xx_train, yy_train)\n",
        "\n",
        "print(\"\\nDesempenho com melhores parâmetros (CV KFold):\")\n",
        "print(best_params)\n",
        "print(f\"RMSE médio: {rmse_mean:.4f} km/h\")\n",
        "print(f\"RMSE std:   {rmse_std:.4f} km/h\")\n",
        "print(f\"R² médio:   {r2_mean:.4f}\")\n",
        "print(f\"R² std:     {r2_std:.4f}\")"
      ],
      "metadata": {
        "id": "lNGvSVhz37bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14.3: Avaliando sem as features contextuais"
      ],
      "metadata": {
        "id": "Df8dfr5cgZgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_params(params, X, y, n_splits=5, random_state=42):\n",
        "  kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "  rmses = []\n",
        "  r2s = []\n",
        "  for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "    X_tr_raw = X.iloc[train_idx].copy()\n",
        "    X_val_raw = X.iloc[val_idx].copy()\n",
        "    y_tr = y.iloc[train_idx].copy()\n",
        "    y_val = y.iloc[val_idx].copy()\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_tr_scaled_num = scaler.fit_transform(X_tr_raw[columns_to_scale])\n",
        "    X_val_scaled_num = scaler.transform(X_val_raw[columns_to_scale])\n",
        "    X_tr_scaled = pd.DataFrame(X_tr_scaled_num, columns=columns_to_scale, index=X_tr_raw.index)\n",
        "    X_val_scaled = pd.DataFrame(X_val_scaled_num, columns=columns_to_scale, index=X_val_raw.index)\n",
        "    X_tr_scaled = pd.concat([X_tr_scaled, X_tr_raw[excluded_columns]], axis=1)\n",
        "    X_val_scaled = pd.concat([X_val_scaled, X_val_raw[excluded_columns]], axis=1)\n",
        "\n",
        "    X_tr_scaled = X_tr_scaled[all_features]\n",
        "    X_val_scaled = X_val_scaled[all_features]\n",
        "\n",
        "    xgb_model = xgb.XGBRegressor( n_estimators=params['n_estimators'], max_depth=params['max_depth'], learning_rate=params['learning_rate'],\n",
        "                                subsample=params['subsample'], colsample_bytree=params['colsample_bytree'], min_child_weight=params['min_child_weight'],\n",
        "                                  gamma=params['gamma'], reg_alpha=params['reg_alpha'], reg_lambda=params['reg_lambda'],random_state=42, n_jobs=-1, tree_method=\"hist\")\n",
        "    xgb_model.fit(X_tr_scaled, y_tr)\n",
        "    y_pred = xgb_model.predict(X_val_scaled)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    rmses.append(rmse)\n",
        "    r2s.append(r2)\n",
        "  rmse_mean = np.mean(rmses)\n",
        "  rmse_std = np.std(rmses)\n",
        "  r2_mean = np.mean(r2s)\n",
        "  r2_std = np.std(r2s)\n",
        "  return rmse_mean, rmse_std, r2_mean, r2_std\n"
      ],
      "metadata": {
        "id": "0j9LoVuiInPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols=['PCI', 'hour_sin', 'hour_cos', 'day_of_week','environment', 'Altitude', 'distance_w']\n",
        "Xx_train = Xx_train.drop(cols,axis=1)\n",
        "excluded_columns = ['SSBIdx']\n",
        "\n",
        "all_features = Xx_train.columns.tolist()\n",
        "columns_to_scale = [c for c in all_features if c not in excluded_columns]\n"
      ],
      "metadata": {
        "id": "x6UTx89lJs0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(\n",
        "    direction='minimize',\n",
        "    sampler=TPESampler(seed=42)\n",
        ")\n",
        "\n",
        "study.enqueue_trial({\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 15,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8\n",
        "})\n",
        "\n",
        "\n",
        "study.optimize(objective, timeout=3600)\n",
        "\n",
        "print(\"Melhores parâmetros encontrados:\")\n",
        "print(study.best_params)\n",
        "\n",
        "best_params = study.best_params\n",
        "rmse_mean, rmse_std, r2_mean, r2_std = evaluate_params(best_params, Xx_train, yy_train)\n",
        "\n",
        "print(\"\\nDesempenho com melhores parâmetros (CV KFold):\")\n",
        "print(best_params)\n",
        "print(f\"RMSE médio: {rmse_mean:.4f} km/h\")\n",
        "print(f\"RMSE std:   {rmse_std:.4f} km/h\")\n",
        "print(f\"R² médio:   {r2_mean:.4f}\")\n",
        "print(f\"R² std:     {r2_std:.4f}\")"
      ],
      "metadata": {
        "id": "0h_eU7oYJUZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Teste Final dos melhores modelos"
      ],
      "metadata": {
        "id": "s90qVpNNho2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15.1: Teste final com todas as features contextuais"
      ],
      "metadata": {
        "id": "HdMb4svmjCk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col=df_best2.columns.to_list()\n",
        "col.remove('Speed')\n",
        "X = df_best2[col].copy()\n",
        "y = df_best2['Speed'].copy()\n",
        "\n",
        "Xx_train, X_test, yy_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "excluded_non_transformed_columns = ['day_of_week', 'PCI', 'SSBIdx']\n",
        "one_hot_encode_columns = ['environment']\n",
        "\n",
        "columns_to_scale = [c for c in col if c not in excluded_non_transformed_columns + one_hot_encode_columns]\n",
        "\n",
        "\n",
        "X_train_to_scale = Xx_train[columns_to_scale]\n",
        "X_train_to_onehot = Xx_train[one_hot_encode_columns]\n",
        "X_train_non_transformed = Xx_train[excluded_non_transformed_columns]\n",
        "\n",
        "X_test_to_scale_final = X_test[columns_to_scale]\n",
        "X_test_to_onehot_final = X_test[one_hot_encode_columns]\n",
        "X_test_non_transformed_final = X_test[excluded_non_transformed_columns]\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled_array = scaler.fit_transform(X_train_to_scale)\n",
        "X_test_scaled_array = scaler.transform(X_test_to_scale_final)\n",
        "\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled_array, columns=columns_to_scale, index=Xx_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled_array, columns=columns_to_scale, index=X_test.index)\n",
        "\n",
        "\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "X_train_ohe_array = ohe.fit_transform(X_train_to_onehot)\n",
        "X_test_ohe_array = ohe.transform(X_test_to_onehot_final)\n",
        "\n",
        "ohe_feature_names = ohe.get_feature_names_out(one_hot_encode_columns)\n",
        "\n",
        "X_train_ohe_df = pd.DataFrame(X_train_ohe_array, columns=ohe_feature_names, index=Xx_train.index)\n",
        "X_test_ohe_df = pd.DataFrame(X_test_ohe_array, columns=ohe_feature_names, index=X_test.index)\n",
        "\n",
        "\n",
        "X_train_scaled = pd.concat([X_train_scaled_df, X_train_ohe_df, X_train_non_transformed], axis=1)\n",
        "X_test_scaled = pd.concat([X_test_scaled_df, X_test_ohe_df, X_test_non_transformed_final], axis=1)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODELO: XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "xgb_model_final1 = xgb.XGBRegressor(\n",
        "    n_estimators=150,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.07,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.7,\n",
        "    min_child_weight=1,\n",
        "    gamma=0.1,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando XGBoost...\")\n",
        "xgb_model_final1.fit(X_train_scaled, yy_train, verbose=False) # Train on yy_train\n",
        "\n",
        "y_pred_xgb_fnal1 = xgb_model_final1.predict(X_test_scaled)\n",
        "\n",
        "xgb_final1_r2 = r2_score(y_test, y_pred_xgb_fnal1)\n",
        "xgb_final1_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_fnal1))\n",
        "xgb_final1_mae = mean_absolute_error(y_test, y_pred_xgb_fnal1)\n",
        "\n",
        "print(f\"\\nR²: {xgb_final1_r2:.4f}\")\n",
        "print(f\"RMSE: {xgb_final1_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {xgb_final1_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "yJGehwLHlFm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15.2: Teste final eliminando a feature de ambiente"
      ],
      "metadata": {
        "id": "zCq_JOTNl0YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_best2=df_best2.drop('Altitude',axis=1)\n",
        "col=df_best2.columns.to_list()\n",
        "col.remove('Speed')\n",
        "X = df_best2[col].copy()\n",
        "y = df_best2['Speed'].copy()\n",
        "\n",
        "Xx_train, X_test, yy_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "excluded_non_transformed_columns = ['day_of_week', 'PCI', 'SSBIdx']\n",
        "one_hot_encode_columns = ['environment']\n",
        "\n",
        "columns_to_scale = [c for c in col if c not in excluded_non_transformed_columns + one_hot_encode_columns]\n",
        "\n",
        "\n",
        "X_train_to_scale = Xx_train[columns_to_scale]\n",
        "X_train_to_onehot = Xx_train[one_hot_encode_columns]\n",
        "X_train_non_transformed = Xx_train[excluded_non_transformed_columns]\n",
        "\n",
        "X_test_to_scale_final = X_test[columns_to_scale]\n",
        "X_test_to_onehot_final = X_test[one_hot_encode_columns]\n",
        "X_test_non_transformed_final = X_test[excluded_non_transformed_columns]\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled_array = scaler.fit_transform(X_train_to_scale)\n",
        "X_test_scaled_array = scaler.transform(X_test_to_scale_final)\n",
        "\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled_array, columns=columns_to_scale, index=Xx_train.index)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled_array, columns=columns_to_scale, index=X_test.index)\n",
        "\n",
        "\n",
        "\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "X_train_ohe_array = ohe.fit_transform(X_train_to_onehot)\n",
        "X_test_ohe_array = ohe.transform(X_test_to_onehot_final)\n",
        "\n",
        "ohe_feature_names = ohe.get_feature_names_out(one_hot_encode_columns)\n",
        "\n",
        "X_train_ohe_df = pd.DataFrame(X_train_ohe_array, columns=ohe_feature_names, index=Xx_train.index)\n",
        "X_test_ohe_df = pd.DataFrame(X_test_ohe_array, columns=ohe_feature_names, index=X_test.index)\n",
        "\n",
        "\n",
        "X_train_scaled = pd.concat([X_train_scaled_df, X_train_ohe_df, X_train_non_transformed], axis=1)\n",
        "X_test_scaled = pd.concat([X_test_scaled_df, X_test_ohe_df, X_test_non_transformed_final], axis=1)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODELO: XGBOOST\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "xgb_model_final1 = xgb.XGBRegressor(\n",
        "    n_estimators=150,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.07,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.6,\n",
        "    min_child_weight=1,\n",
        "    gamma=0.1,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando XGBoost...\")\n",
        "xgb_model_final1.fit(X_train_scaled, yy_train, verbose=False) # Train on yy_train\n",
        "\n",
        "y_pred_xgb_fnal1 = xgb_model_final1.predict(X_test_scaled)\n",
        "\n",
        "xgb_final1_r2 = r2_score(y_test, y_pred_xgb_fnal1)\n",
        "xgb_final1_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_fnal1))\n",
        "xgb_final1_mae = mean_absolute_error(y_test, y_pred_xgb_fnal1)\n",
        "\n",
        "print(f\"\\nR²: {xgb_final1_r2:.4f}\")\n",
        "print(f\"RMSE: {xgb_final1_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {xgb_final1_mae:.4f} km/h\")"
      ],
      "metadata": {
        "id": "QwxNtTszl4Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15.3: Teste final eliminando as features contextuais"
      ],
      "metadata": {
        "id": "6ESVfFBTmxgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col1=['PCI', 'hour_sin', 'hour_cos', 'day_of_week','environment',  'distance_w']\n",
        "df_best2=df_best2.drop(col1,axis=1)\n",
        "col=df_best2.columns.to_list()\n",
        "col.remove('Speed')\n",
        "X = df_best2[col].copy()\n",
        "y = df_best2['Speed'].copy()\n",
        "\n",
        "Xx_train, X_test, yy_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "excluded_columns_for_scaling = ['SSBIdx']\n",
        "\n",
        "columns_to_scale = [c for c in col if c not in excluded_columns_for_scaling]\n",
        "\n",
        "X_train_final_to_scale = Xx_train[columns_to_scale]\n",
        "X_train_final_excluded = Xx_train[excluded_columns_for_scaling]\n",
        "\n",
        "X_test_to_scale_final = X_test[columns_to_scale]\n",
        "X_test_excluded_final = X_test[excluded_columns_for_scaling]\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train_scaled_array = scaler.fit_transform(X_train_final_to_scale)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled_array, columns=columns_to_scale, index=Xx_train.index)\n",
        "X_train_scaled = pd.concat([X_train_scaled, X_train_final_excluded], axis=1)\n",
        "\n",
        "X_test_scaled_array = scaler.transform(X_test_to_scale_final)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled_array, columns=columns_to_scale, index=X_test.index)\n",
        "X_test_scaled = pd.concat([X_test_scaled, X_test_excluded_final], axis=1)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODELO: XGBOOST (SEM FEATURES CONTEXTUAIS)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "xgb_model_final3 = xgb.XGBRegressor(\n",
        "    n_estimators=150,\n",
        "    max_depth=15,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.7,\n",
        "    min_child_weight=1,\n",
        "    gamma=0.1,\n",
        "    reg_alpha=0.01,\n",
        "    reg_lambda=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\nTreinando XGBoost...\")\n",
        "xgb_model_final3.fit(X_train_scaled, yy_train, verbose=False)\n",
        "\n",
        "y_pred_xgb_fnal3 = xgb_model_final3.predict(X_test_scaled)\n",
        "\n",
        "xgb_final3_r2 = r2_score(y_test, y_pred_xgb_fnal3)\n",
        "xgb_final3_rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb_fnal3))\n",
        "xgb_final3_mae = mean_absolute_error(y_test, y_pred_xgb_fnal3)\n",
        "\n",
        "print(f\"\\nR²: {xgb_final3_r2:.4f}\")\n",
        "print(f\"RMSE: {xgb_final3_rmse:.4f} km/h\")\n",
        "print(f\"MAE: {xgb_final3_mae:.4f} km/h\")\n"
      ],
      "metadata": {
        "id": "17ceX2aSnCk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vB9fropHH4KN",
        "eqlZIBVM6ixy",
        "sMFgpvDC6ixz",
        "wuIqYLUb6ix0",
        "3Tqpfuze6ix0",
        "b_b5RXeZ6ix2",
        "RAy-L0Zu19Bj",
        "Pwtw38sst9mA",
        "Zdn4Y6g0uYnB",
        "aAfmWGdPuzrI",
        "WgPk7fQVxbBn",
        "KyMRdM90e5PR",
        "KqgqBYWdflg4",
        "O878A2XHgSLo"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}